{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd53aaaf-9352-47c1-a57b-6e67930dd236",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(igraph): there is no package called ‘igraph’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(igraph): there is no package called ‘igraph’\nTraceback:\n",
      "1. library(igraph)"
     ]
    }
   ],
   "source": [
    "############################ \n",
    "# 1.  Social Network \n",
    "##########################\n",
    "\n",
    "library(igraph)\n",
    "library(RColorBrewer)\n",
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04cd319-6904-46d3-80c4-062a8c7bdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first read in the Enron dataset (stringsAsFactors=F means we do not treat strings as factors)\n",
    "# The nodes data frame contains information about all people in Enron, each treated as a node in our network\n",
    "nodes <- read.csv(\"enron_nodes.csv\", stringsAsFactors=F)\n",
    "str(nodes)\n",
    "# The pairs data frame includes all the pairs of employees who were in frequent communication\n",
    "# The 'ct' column incicates the number of emails sent to each other over a pre-specified period\n",
    "pairs <- read.csv('emails.csv')\n",
    "str(pairs)\n",
    "# As you can see, all the pairs in the dataset have sent at least 10 emails to each other\n",
    "min(pairs$ct)\n",
    "\n",
    "# We now create an undirected graph, where the vertices/nodes come from the 'nodes' variable and the edges come from the 'pairs' dataset\n",
    "enron.G <- graph.data.frame(pairs, directed=FALSE, vertices=nodes)  \n",
    "\n",
    "# Let us now plot the graph\n",
    "# When we plot the graph, there is some randomness regarding where each node will be located\n",
    "# Thereforem we will fix the seed\n",
    "set.seed(144)\n",
    "plot(enron.G, vertex.label=NA, vertex.size=3, vertex.color = \"black\")\n",
    "# From the plot, we see some ``outliers'', i.e., those nodes that do not have any correspondences with other nodes.\n",
    "\n",
    "# We can also add some color to the graph.\n",
    "# For instance, let us color by position in the company\n",
    "color.plot <- rep('white',nrow(nodes)) #we create a vector of values 'white'\n",
    "# We now specify a color for each position in the company\n",
    "color.plot[nodes$Type==\"Trading\"] <- 'blue'\n",
    "color.plot[nodes$Type==\"Legal\"] <- 'green'\n",
    "color.plot[nodes$Type==\"Executive\"] <- 'red'\n",
    "color.plot[nodes$Type==\"Pipeline\"] <- 'brown'\n",
    "color.plot[nodes$Type==\"Risk Management\"] <- 'cyan'\n",
    "color.plot[nodes$Type==\"Regulatory Affairs\"] <- 'black'\n",
    "color.plot[nodes$Type==\"Logistics\"] <- 'purple'\n",
    "color.plot[nodes$Type==\"Accounting\"] <- 'magenta'\n",
    "\n",
    "# Let us apply our colors:\n",
    "set.seed(144)\n",
    "plot(enron.G, vertex.label=NA, vertex.size=3, vertex.color = color.plot)\n",
    "\n",
    "## We first look at the degrees of all the nodes\n",
    "# This is done with the degree() function\n",
    "enron.degree <- degree(enron.G)\n",
    "# Let us look at the output:\n",
    "enron.degree\n",
    "# It is a vector with one value for each employee, which indicates the number of connections of that employee\n",
    "\n",
    "set.seed(144)\n",
    "# We set the node size proportional to its degree for better visualization\n",
    "plot(enron.G, vertex.label=NA, vertex.size=0.3*enron.degree,vertex.color = color.plot)\n",
    "\n",
    "# We display the top three people with the highest degree\n",
    "# The order() function takes the order of indices such that the underlying vector is sorted\n",
    "# The decreasing=TRUE argument specifies that we are sorting the vector in decreasing order\n",
    "nodes[order(enron.degree,decreasing=TRUE)[1:3],]\n",
    "\n",
    "## We then look at the closeness of all the nodes\n",
    "# A challenge here is that closeness is ill defined for disconnected graphs\n",
    "# We therefore decompose the graph into complete sub-graphs\n",
    "# components(enron.G)$csize denotes the size of each subgraph, and which.max() finds the index of the largest one\n",
    "# and components(enron.G)$membership indicates the membership of each node\n",
    "nodes.in.subgraph <- components(enron.G)$membership == which.max(components(enron.G)$csize) # extract the nodes in the largest subgraph\n",
    "subgraph <- induced_subgraph(enron.G, nodes.in.subgraph)  # extract the subgraph from enron.G \n",
    "\n",
    "# We can calculate the closeness as follows:\n",
    "closeness(subgraph)\n",
    "# But note that we are only getting values for the nodes in the main subgraph:\n",
    "length(closeness(subgraph))\n",
    "# So we need to fix a value of 0 for the other ones\n",
    "# We will do that in a 'cl' variable\n",
    "cl <- rep(0, nrow(nodes))           # all the nodes in enron.G intially has closeness 0\n",
    "# only compute the closeness of the nodes in the subgraph; the rest nodes have closeness 0, which makes sense:\n",
    "# if a node is disconnected from the main subgraph, then it takes infinite number of steps to reach other nodes\n",
    "# so its closeness is 0\n",
    "cl[nodes.in.subgraph] <- closeness(subgraph)\n",
    "set.seed(144)\n",
    "plot(enron.G, vertex.label=NA, vertex.size=2000*cl, vertex.color = color.plot)\n",
    "\n",
    "# We display the top three people with the highest closeness centrality\n",
    "nodes[order(cl,decreasing = T)[1:3],]\n",
    "\n",
    "## We now proceed as for \"degree\" for betweenness and pagerank\n",
    "\n",
    "## Betweenness\n",
    "enron.betweenness <- betweenness(enron.G)\n",
    "set.seed(144)\n",
    "plot(enron.G, vertex.label=NA, vertex.size=0.02*enron.betweenness,vertex.color = color.plot)\n",
    "nodes[order(enron.betweenness,decreasing = T)[1:3],]\n",
    "\n",
    "## PageRank\n",
    "page.rank.score = page.rank(enron.G)$vector\n",
    "set.seed(144)\n",
    "# Note that with the page.rank() function, we have multiple outputs, so we need to take the one called 'vector'\n",
    "plot(enron.G, vertex.label=NA, vertex.size=500*page.rank.score,vertex.color = color.plot)\n",
    "nodes[order(page.rank.score,decreasing = T)[1:3],]\n",
    "\n",
    "##### Community detection\n",
    "\n",
    "# igraph makes this pretty easy too!\n",
    "# There are a few ways to cluster communities, and different algorithms\n",
    "# It is a hard problem, so some methods are fast and less accurate, some are more.\n",
    "\n",
    "# We'll use the cluster_spinglass function in this lecture\n",
    "# Due to how the algorithm works, cluster_spinglass only works if we have a single connected component\n",
    "# Therefore, we will now be using our subgraph with 141 nodes rather than the full graph with 148 nodes\n",
    "\n",
    "set.seed(144)\n",
    "spinglass = cluster_spinglass(subgraph, spins = 100)\n",
    "# Note: spins is an integer constant, an upper limit for the number of communities.\n",
    "\n",
    "# Now we have a community for each node (if you set a\n",
    "# different random seed you may have different communities)\n",
    "community = spinglass$membership\n",
    "# This gives you the community to which each node belongs\n",
    "community\n",
    "\n",
    "# We can look at the size of each community as follows\n",
    "table(community)\n",
    "# We obtain 10 communities in total\n",
    "\n",
    "# We can look at the breakdown by position\n",
    "table(nodes$Type[nodes.in.subgraph], community)\n",
    "\n",
    "# And we can also look at the modularity of our community detection:\n",
    "modularity <- spinglass$modularity # modularity = p.within - p.random\n",
    "\n",
    "# Finally, we will plot the subgraph\n",
    "# We will include a different color for each community\n",
    "#Remember: We have 10 communities:\n",
    "max(community)\n",
    "# So we will define 10 colors\n",
    "color.communities <- brewer.pal(max(community),\"Spectral\")\n",
    "# The 'brewer.pal' function defines a palette of the pre-specified number of colors\n",
    "\n",
    "# Let's plot!\n",
    "set.seed(144)\n",
    "plot(subgraph, vertex.label=NA, vertex.size=5, vertex.color=color.communities[community])\n",
    "\n",
    "## Now, we include \"ground truth\" communities, based on job positions\n",
    "\n",
    "set.seed(144)\n",
    "plot(subgraph, vertex.label=NA, vertex.size=ifelse(V(subgraph)$Type == \"Trading\", 8, 3),# if the employee does trading, the node has size 10; and 3 otherwise \n",
    "     vertex.color=color.communities[community])\n",
    "\n",
    "set.seed(144)\n",
    "plot(subgraph, vertex.label=NA, vertex.size=ifelse(V(subgraph)$Type == \"Trading\" & V(subgraph)$Region==\"W\", 8, 3),# if the employee does trading and in west region, the node has size 10; and 3 otherwise \n",
    "     vertex.color=color.communities[community])\n",
    "\n",
    "set.seed(144)\n",
    "plot(subgraph, vertex.label=NA, vertex.size=ifelse(V(subgraph)$Type == \"Executive\", 8, 3),# if the employee does executive jobs, the node has size 10; and 3 otherwise \n",
    "     vertex.color=color.communities[community])\n",
    "\n",
    "set.seed(144)\n",
    "plot(subgraph, vertex.label=NA, vertex.size=ifelse(V(subgraph)$Type == \"Pipeline\", 10, 3),# if the employee does pipeline jobs, the node has size 10; and 3 otherwise \n",
    "     vertex.color=color.communities[community])\n",
    "\n",
    "set.seed(144)\n",
    "plot(subgraph, vertex.label=NA, vertex.size=ifelse(V(subgraph)$Type == \"Accounting\", 10, 3),# if the employee does accounting, the node has size 10; and 3 otherwise \n",
    "     vertex.color=color.communities[community])\n",
    "\n",
    "set.seed(144)\n",
    "plot(subgraph, vertex.label=NA, vertex.size=ifelse(V(subgraph)$Type == \"Legal\", 10, 3),# if the employee does legal jobs, the node has size 10; and 3 otherwise \n",
    "     vertex.color=color.communities[community])\n",
    "\n",
    "############################################################\n",
    "# 2. Collaborative filtering for building recommender systems.\n",
    "############################################################\n",
    "\n",
    "library(softImpute) # collaborative filtering package\n",
    "\n",
    "# The following command reads the MovieLens data.\n",
    "ratings <- read.csv('ratings.csv')\n",
    "\n",
    "max(ratings$movieID)\n",
    "\n",
    "# Let's investigate the dataset...\n",
    "head(ratings)\n",
    "# The first column lists the user IDs\n",
    "# The second column lists the movie IDs\n",
    "# The third column lists the ratings\n",
    "# The next columns provides the time of the rating\n",
    "\n",
    "library(plyr)\n",
    "count(ratings, \"userID\")\n",
    "\n",
    "# Let us split the dataset into a training set and a test set, as usual\n",
    "# The overwhelming majority (99%) of the dataset will go into the training set.\n",
    "# This is because the data are already very sparse, so we want to use as much of it as possible\n",
    "set.seed(144)\n",
    "train.idx <- sample(seq_len(nrow(ratings)), 0.99*nrow(ratings))\n",
    "train.ratings <- ratings[train.idx,]\n",
    "test.ratings <- ratings[-train.idx,]\n",
    "\n",
    "# Let us transform the training set into a matrix.\n",
    "# The rows correspond to users\n",
    "# The columns correspond to movies\n",
    "# The entries correspond to ratings\n",
    "# The issue is that ratings are missing for a lot of user-movie pairs (this is precisely what we want to estimate!)\n",
    "# This is accomplished using the \"Incomplete\" function\n",
    "mat.train <- Incomplete(train.ratings[, 1], train.ratings[, 2], train.ratings[, 3]) #(user, movie, rating)\n",
    "dim(train.ratings)\n",
    "# Let us see... Each dot repressents a missing entry. There are a lot of them!\n",
    "mat.train\n",
    "\n",
    "# We are now ready to apply our collaborative filtering algorithm.\n",
    "# This is done with the softImpute function, as follows:\n",
    "set.seed(144)\n",
    "fit <- softImpute(mat.train, rank.max=9, lambda=0, maxit=1000)\n",
    "# The \"rank.max\" argument corresponds to the maximal number of archetypes (in lecture, we called it \"k\")\n",
    "# Let's not worry about lambda for now (it has to do with the Lasso procedure---stay tuned!)\n",
    "# The \"maxit\" argument specifies a maximum number of iterations for the fitting procedure\n",
    "\n",
    "# There are three primary outputs to the softImpute function\n",
    "# First, the \"u\" element is a matrix of size \"number of users\" x \"number of archetypes\"\n",
    "# It represents the linear combination of archetypes for each user\n",
    "fit$u   # n x k (#user x #archetype)\n",
    "# The other two elements are called \"v\" and \"d\"\n",
    "# \"v\" is of size \"number of movies\" * \"number of archetypes\"; it indicates the relative preference of each archetype for each movie\n",
    "# \"d\" is of size \"number of archetypes\"; it is just a scaling factor (diagonal matrix)\n",
    "# Their product is of size \"number of movies\" * \"number of archetypes\"\n",
    "# It indicates the rating of each archetype for each movie\n",
    "fit$v * fit$d  # m x k  (#movie x #archetype)\n",
    "\n",
    "# We are now ready to make predictions!\n",
    "# For that, we need a list of users (test.ratings[, 1]) and a list of movies (test.ratings[, 2])\n",
    "# We call the function \"impute\"\n",
    "pred.insample.0 <- impute(fit, train.ratings[, 1], train.ratings[, 2])\n",
    "pred.outsample.0 <- impute(fit, test.ratings[, 1], test.ratings[, 2])\n",
    "\n",
    "# You can verify manually that the predicted value is just the sumproduct of the weights (fit$u) and the archetype ratings (fit$v * fit*d)\n",
    "# sum(fit$u[test.ratings[1,1],] * fit$v[test.ratings[1,2],] * fit$d)\n",
    "# pred.outsample.0[1]\n",
    "\n",
    "# Let us show the histogram of these values\n",
    "hist(pred.insample.0)\n",
    "hist(pred.outsample.0)\n",
    "\n",
    "# The issue is that some values are lower than 1 and some are higher than 5.\n",
    "# We simply treat all values lower than 1 as 1's and all values higher than 5 as 5's.\n",
    "# This is done as follows:\n",
    "pred.insample <- pmax(pmin(pred.insample.0, 5), 1)\n",
    "pred.outsample <- pmax(pmin(pred.outsample.0, 5), 1)\n",
    "\n",
    "# We now compute the in-sample and out-of-sample performance of the model:\n",
    "R2.insample <- 1 - sum((pred.insample-train.ratings$rating)^2)/sum((mean(train.ratings$rating) - train.ratings$rating)^2)\n",
    "R2.outsample <- 1 - sum((pred.outsample-test.ratings$rating)^2)/sum((mean(train.ratings$rating) - test.ratings$rating)^2)\n",
    "R2.insample\n",
    "R2.outsample\n",
    "\n",
    "############################################################\n",
    "# 3. Ensemble modeling for enhancing recommender systems. (Optional)\n",
    "############################################################\n",
    "\n",
    "# As discussed in class, the prediction from the collaborative filtering model can be integrated into a regression model\n",
    "# In other words, the regression model will leverage the outputs of the collaborative filtering model, as well as additional features on the users and the movies\n",
    "# By doing so, we combine two predictive methods together. This is called ensemble modeling.\n",
    "\n",
    "# Let us first import additional data\n",
    "\n",
    "### USER DATA\n",
    "user.info <- read.csv(\"users.csv\")\n",
    "\n",
    "# Let's investigate the dataset...\n",
    "head(user.info)\n",
    "str(user.info)\n",
    "# The first column lists the user IDs\n",
    "# The second column lists the age\n",
    "# The third column lists the job category\n",
    "# The fourth column lists the zipcode\n",
    "# The last column lists the gender (1 for male, 0 for female)\n",
    "\n",
    "### ZIPCODE-INCOME DATA\n",
    "zip.med <- read.csv(\"MedianZIP-3.csv\")\n",
    "\n",
    "# Let's investigate the dataset...\n",
    "head(zip.med)\n",
    "str(zip.med)\n",
    "# The first column lists the zipcodes\n",
    "# The second column lists the median income\n",
    "# The third column lists the mean income\n",
    "# The fourth column lists the population\n",
    "\n",
    "# A bit of data cleaning...\n",
    "# Remove commas in numbers and convert into numbers\n",
    "zip.med$Median <- as.numeric(gsub(\",\", \"\", zip.med$Median))\n",
    "# Make sure all zip codes have 5 digits\n",
    "zip.med$Zip <- str_pad(zip.med$Zip, 5, \"left\", \"0\")\n",
    "# Match income from zip.med with the zipcodes given in user.info\n",
    "# The challenge is that, due to missing data entries, we will have \"NA\" values\n",
    "# For these, we input the average median income, taken over all zipcodes for which we have values\n",
    "user.info$MedianIncome <- NA                # new field\n",
    "m <- match(user.info$ZipCode, zip.med$Zip)\n",
    "user.info$MedianIncome[!is.na(m)] <- zip.med$Median[m[!is.na(m)]]\n",
    "user.info$MedianIncome[is.na(user.info$MedianIncome)] <- mean(user.info$MedianIncome, na.rm=TRUE) # ave median income\n",
    "\n",
    "# In the model presented in class, we also included additional user-level features\n",
    "# by mapping zipcode into an urban/rural indicator and into a region (Pacific, Eastern, etc.)\n",
    "# Here, we simplify the model by ignoring these additional data (which were not the main drivers of the predictions anyway)\n",
    "\n",
    "### MOVIE GENRE DATA\n",
    "genre.mat <- read.csv('genre.csv')\n",
    "\n",
    "# Let us investigate the dataset\n",
    "head(genre.mat)\n",
    "# Each row lists a movie (the row ids correspond to the movie ids in the ratings.csv file)\n",
    "# Each column lists a genre\n",
    "# a value of 1 means that the movie is classified into the genre\n",
    "\n",
    "# Let us combine all this information into a model matrix\n",
    "# The function model.matrix() works similarly to the regression functions, except that it does not fit any model---it just creates the matrix\n",
    "# Here, we consider all the variables, except zipcode itself (we used it to get acces to the median income data, but zipcode itself is not informative)\n",
    "MM <- as.data.frame(model.matrix(~.-ZipCode, data=user.info)[,-(1:2)])  # exclude the first two cols\n",
    "\n",
    "# Let us investigate this matrix\n",
    "head(MM)\n",
    "\n",
    "### Re-construction of a training set and a test set with all the relevant variables, including:\n",
    "# the user info data\n",
    "# the movie genre data\n",
    "# the rating data,\n",
    "# the previous predictions from the collaborative filtering ('CF') model, and\n",
    "# some additional data fields, such as weekday, month, year, and hour of day\n",
    "full.mat <- cbind(MM[ratings$userID,], genre.mat[ratings$movieID,])  # see ratings\n",
    "full.train <- full.mat[train.idx,]   # previous training indices\n",
    "full.train$CF <- pred.insample       # new field\n",
    "full.train <- cbind(train.ratings[,c(\"rating\", \"wday\", \"mon\", \"year\", \"hour\")],full.train) # prepend date info\n",
    "names(full.train) <- make.names(names(full.train)) # ensure names to be syntatically correct\n",
    "\n",
    "full.test <- full.mat[-train.idx,]\n",
    "full.test$CF <- pred.outsample\n",
    "full.test <- cbind(test.ratings[,c(\"rating\", \"wday\", \"mon\", \"year\", \"hour\")],full.test)\n",
    "names(full.test) <- make.names(names(full.test))\n",
    "\n",
    "# We now run our linear regression model, using all the aforementioned predictors:\n",
    "mod.lm.full <- lm(rating~., data=full.train)\n",
    "summary(mod.lm.full)\n",
    "R2.insample\n",
    "\n",
    "# Our previous in-sample R2 was 0.4256. Our new in-sample R2 is 0.4352.\n",
    "# This is not surprising:\n",
    "# The linear regression model could \"just\" use the results of the collaborative filtering and ignore all the other independent variables.\n",
    "# Since the linear regression model has access to so many other independent variables, it can do strictly better ON THE TRAINING SET\n",
    "# What about the test set? Let us find out...\n",
    "\n",
    "pred.lm.full <- predict(mod.lm.full, newdata=full.test)\n",
    "R2.lm.full <- 1 - sum((pred.lm.full-full.test$rating)^2)/sum((mean(full.train$rating) - full.test$rating)^2)\n",
    "R2.lm.full\n",
    "R2.outsample\n",
    "\n",
    "# The ensemble model does better than \"just\" the collaborative filtering model ON THE TEST SET\n",
    "# The values reported here are slightly worse than those from the lecture, because we only used a subset of the independent variables.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
